{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemma = WordNetLemmatizer()\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\91983\\Desktop\\NLP\\dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking counts of all the languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thai          1000\n",
       "Tamil         1000\n",
       "Spanish       1000\n",
       "Romanian      1000\n",
       "Persian       1000\n",
       "French        1000\n",
       "Russian       1000\n",
       "Hindi         1000\n",
       "Estonian      1000\n",
       "Urdu          1000\n",
       "Dutch         1000\n",
       "Pushto        1000\n",
       "Chinese       1000\n",
       "Japanese      1000\n",
       "Indonesian    1000\n",
       "Arabic        1000\n",
       "English       1000\n",
       "Korean        1000\n",
       "Latin         1000\n",
       "Swedish       1000\n",
       "Turkish       1000\n",
       "Portugese     1000\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "language    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test=train_test_split(data,test_size=0.25,random_state=2,stratify=data['language'])\n",
    "\n",
    "train=train.reset_index()\n",
    "train.drop('index',1,inplace=True)\n",
    "test=test.reset_index()\n",
    "test.drop('index',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_alpha=train[(train['language']=='Estonian') | (train['language']=='Swedish') | (train['language']=='Dutch') | (train['language']=='Turkish')| (train['language']=='Latin') |  (train['language']=='Indonesian') | (train['language']=='Portugese') | (train['language']=='French') |  (train['language']=='Spanish') |  (train['language']=='Romanian') | (train['language']=='Russian') | (train['language']=='English')].reset_index(drop=True) \n",
    "train_nonalpha=train[(train['language']=='Thai') | (train['language']=='Tamil') | (train['language']=='Japanese') | (train['language']=='Turkish')| (train['language']=='Urdu') |  (train['language']=='Chinese') | (train['language']=='Korean') | (train['language']=='Hindi') |  (train['language']=='Pushto') |  (train['language']=='Persian') | (train['language']=='Arabic')].reset_index(drop=True) \n",
    "\n",
    "\n",
    "test_alpha=test[(test['language']=='Estonian') | (test['language']=='Swedish') | (test['language']=='Dutch') | (test['language']=='Turkish')| (test['language']=='Latin') |  (test['language']=='Indonesian') | (test['language']=='Portugese') | (test['language']=='French') |  (test['language']=='Spanish') |  (test['language']=='Romanian') | (test['language']=='Russian') | (test['language']=='English')].reset_index(drop=True) \n",
    "test_nonalpha=test[(test['language']=='Thai') | (test['language']=='Tamil') | (test['language']=='Japanese') | (test['language']=='Urdu') |  (test['language']=='Chinese') | (test['language']=='Korean') | (test['language']=='Hindi') |  (test['language']=='Pushto') |  (test['language']=='Persian') | (test['language']=='Arabic')].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train=[]\n",
    "\n",
    "for i in range(len(train_alpha)):\n",
    "    review_alpha=re.sub('[0-9]','',train_alpha['Text'][i])\n",
    "    review_alpha=review_alpha.lower()\n",
    "    review_alpha=review_alpha.translate(str.maketrans('', '', punctuation))\n",
    "    corpus_train.append(review_alpha)\n",
    "    \n",
    "for i in range(len(train_nonalpha)):\n",
    "    review_nonalpha=re.sub('[0-9A-za-z]','',train_nonalpha['Text'][i])\n",
    "    review_nonalpha=review_nonalpha.lower()\n",
    "    review_nonalpha=review_nonalpha.translate(str.maketrans('', '', punctuation))\n",
    "    corpus_train.append(review_nonalpha)\n",
    "    \n",
    "corpus_test=[]\n",
    "\n",
    "for i in range(len(test_alpha)):\n",
    "    review_alpha=re.sub('[0-9]','',test_alpha['Text'][i])\n",
    "    review_alpha=review_alpha.lower()\n",
    "    review_alpha=review_alpha.translate(str.maketrans('', '', punctuation))\n",
    "    corpus_test.append(review_alpha)    \n",
    "    \n",
    "for i in range(len(test_nonalpha)):\n",
    "    review_nonalpha=re.sub('[0-9]','',test_nonalpha['Text'][i])\n",
    "    review_nonalpha=review_nonalpha.lower()\n",
    "    review_nonalpha=review_nonalpha.translate(str.maketrans('', '', punctuation))\n",
    "    corpus_test.append(review_nonalpha)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_language=pd.concat([train_alpha['language'],train_nonalpha['language']],0)\n",
    "test_language=pd.concat([test_alpha['language'],test_nonalpha['language']],0)\n",
    "\n",
    "\n",
    "train_language=le.fit_transform(train_language)\n",
    "test_language=le.fit_transform(test_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "\n",
    "\n",
    "unigram=CountVectorizer( strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                          stop_words=None, ngram_range=(1,1), max_features=10000)\n",
    "\n",
    "\n",
    "bag_unigram=unigram.fit_transform(corpus_train)\n",
    "\n",
    "\n",
    "bigram=CountVectorizer( strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                          stop_words=None, ngram_range=(2,2), max_features=10000)\n",
    "\n",
    "\n",
    "bag_bigram=bigram.fit_transform(corpus_train)\n",
    "\n",
    "\n",
    "char2gram=CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                          stop_words=None, ngram_range=(2,2), max_features=10000)\n",
    "\n",
    "\n",
    "bag_char2gram=char2gram.fit_transform(corpus_train)\n",
    "\n",
    "\n",
    "char3gram=CountVectorizer( strip_accents='unicode', analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                          stop_words=None, ngram_range=(3,3), max_features=10000)\n",
    "\n",
    "\n",
    "bag_char3gram=char3gram.fit_transform(corpus_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making Data Frames for Unigram Train Data\n",
    "unigram_train=pd.DataFrame(bag_unigram.toarray(),columns=unigram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for Bigram Train Data\n",
    "bigram_train=pd.DataFrame(bag_bigram.toarray(),columns=bigram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for char2gram Train Data\n",
    "char2gram_train=pd.DataFrame(bag_char2gram.toarray(),columns=char2gram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for char3gram Train Data\n",
    "char3gram_train=pd.DataFrame(bag_char3gram.toarray(),columns=char3gram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making Data Frames for Unigram Test Data\n",
    "bag_unigram_test=unigram.transform(corpus_test)\n",
    "unigram_test=pd.DataFrame(bag_unigram_test.toarray(),columns=unigram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for Bigram Test Data\n",
    "bag_bigram_test=bigram.transform(corpus_test)\n",
    "bigram_test=pd.DataFrame(bag_bigram_test.toarray(),columns=bigram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for char2gram Test Data\n",
    "bag_char2gram_test=char2gram.transform(corpus_test)\n",
    "char2gram_test=pd.DataFrame(bag_char2gram_test.toarray(),columns=char2gram.get_feature_names())\n",
    "\n",
    "## Making Data Frames for char3gram Test Data\n",
    "bag_char3gram_test=char3gram.transform(corpus_test)\n",
    "char3gram_test=pd.DataFrame(bag_char3gram_test.toarray(),columns=char3gram.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training With RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators':[100,200,250],\n",
    "       'max_depth':[4,6,7,9],\n",
    "       'criterion':['gini'],\n",
    "       'class_weight':['balanced'],\n",
    "       'min_samples_split':[2,3,5,7],\n",
    "       'min_samples_leaf':[2,3,5,6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_uni_model=RandomForestClassifier()\n",
    "rf_bi_model=RandomForestClassifier()\n",
    "rf_char2_model=RandomForestClassifier()\n",
    "rf_char3_model=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_uni_search=RandomizedSearchCV(rf_uni_model,cv=10,scoring='accuracy',param_distributions=params,verbose=20,n_iter=50,n_jobs=4)\n",
    "rf_bi_search=RandomizedSearchCV(rf_bi_model,cv=10,scoring='accuracy',param_distributions=params,verbose=20,n_iter=50,n_jobs=4)\n",
    "rf_char2_search=RandomizedSearchCV(rf_char2_model,cv=10,scoring='accuracy',param_distributions=params,verbose=20,n_iter=50,n_jobs=4)\n",
    "rf_char3_search=RandomizedSearchCV(rf_char3_model,cv=10,scoring='accuracy',param_distributions=params,verbose=20,n_iter=50,n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unigram Model training\n",
    "rf_uni_search.fit(unigram_train,train_language)\n",
    "rf_unigram=rf_uni_search.best_estimator_\n",
    "\n",
    "print('rf_unigram model fitting Completed')\n",
    "\n",
    "##Bigram Model training\n",
    "rf_bi_search.fit(bigram_train,train_language)\n",
    "rf_bigram=rf_bi_search.best_estimator_\n",
    "\n",
    "print('rf_bigram model fitting Completed')\n",
    "\n",
    "##Char2gram Model training\n",
    "rf_char2_search.fit(char2gram_train,train_language)\n",
    "rf_char2gram=rf_char2_search.best_estimator_\n",
    "\n",
    "print('rf_char2gram model fitting Completed')\n",
    "\n",
    "##Char3gram Model training\n",
    "rf_char3_search.fit(char3gram_train,train_language)\n",
    "rf_char3gram=rf_char3_search.best_estimator_\n",
    "\n",
    "print('rf_char3gram model fitting Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prediction For Unigram Test Data\n",
    "rf_unigram_test_prediction=rf_unigram.predict(unigram_test)\n",
    "\n",
    "##Prediction For Bigram Test Data\n",
    "rf_bigram_test_prediction=rf_bigram.predict(bigram_test)\n",
    "\n",
    "#Prediction For char2gram Test Data\n",
    "rf_char2gram_test_prediction=rf_char2gram.predict(char2gram_test)\n",
    "\n",
    "##Prediction For char3gram Test Data\n",
    "rf_char3gram_test_prediction=rf_char3gram.predict(char3gram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Test Prediction Accuracy with RandomForest Classifer :- 0.902\n",
      "---------------------------------------------------------\n",
      "Bigram Test Prediction Accuracy with RandomForest Classifer :- 0.49127272727272725\n",
      "---------------------------------------------------------\n",
      "Char2Gram Test Prediction Accuracy with RandomForest Classifer :- 0.972\n",
      "---------------------------------------------------------\n",
      "Char3Gram Test Prediction Accuracy with RandomForest Classifer :- 0.9581818181818181\n"
     ]
    }
   ],
   "source": [
    "print('Unigram Test Prediction Accuracy with RandomForest Classifer :-',accuracy_score(test_language,rf_unigram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Bigram Test Prediction Accuracy with RandomForest Classifer :-',accuracy_score(test_language,rf_bigram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Char2Gram Test Prediction Accuracy with RandomForest Classifer :-',accuracy_score(test_language,rf_char2gram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Char3Gram Test Prediction Accuracy with RandomForest Classifer :-',accuracy_score(test_language,rf_char3gram_test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training With Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_uni=MultinomialNB()\n",
    "nb_bi=MultinomialNB()\n",
    "nb_char2=MultinomialNB()\n",
    "nb_char3=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_unigram model fitting Completed\n",
      "nb_bigram model fitting Completed\n",
      "nb_char2gram model fitting Completed\n",
      "nb_char3gram model fitting Completed\n"
     ]
    }
   ],
   "source": [
    "##Unigram Model training\n",
    "nb_unigram=nb_uni.fit(unigram_train,train_language)\n",
    "\n",
    "\n",
    "print('nb_unigram model fitting Completed')\n",
    "\n",
    "##Bigram Model training\n",
    "nb_bigram=nb_bi.fit(bigram_train,train_language)\n",
    "\n",
    "\n",
    "print('nb_bigram model fitting Completed')\n",
    "\n",
    "##Char2gram Model training\n",
    "nb_char2gram=nb_char2.fit(char2gram_train,train_language)\n",
    "\n",
    "\n",
    "print('nb_char2gram model fitting Completed')\n",
    "\n",
    "##Char3gram Model training\n",
    "nb_char3gram=nb_char3.fit(char3gram_train,train_language)\n",
    "\n",
    "\n",
    "print('nb_char3gram model fitting Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prediction For Unigram Test Data\n",
    "nb_unigram_test_prediction=nb_unigram.predict(unigram_test)\n",
    "\n",
    "##Prediction For Bigram Test Data\n",
    "nb_bigram_test_prediction=nb_bigram.predict(bigram_test)\n",
    "\n",
    "#Prediction For char2gram Test Data\n",
    "nb_char2gram_test_prediction=nb_char2gram.predict(char2gram_test)\n",
    "\n",
    "##Prediction For char3gram Test Data\n",
    "nb_char3gram_test_prediction=nb_char3gram.predict(char3gram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Test Prediction Accuracy with Multinomial NB :- 0.9243636363636364\n",
      "---------------------------------------------------------\n",
      "Bigram Test Prediction Accuracy with Multinomial NB :- 0.7854545454545454\n",
      "---------------------------------------------------------\n",
      "Char2Gram Test Prediction Accuracy with Multinomial NB :- 0.9681818181818181\n",
      "---------------------------------------------------------\n",
      "Char3Gram Test Prediction Accuracy with Multinomial NB :- 0.9383636363636364\n"
     ]
    }
   ],
   "source": [
    "print('Unigram Test Prediction Accuracy with Multinomial NB :-',accuracy_score(test_language,nb_unigram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Bigram Test Prediction Accuracy with Multinomial NB :-',accuracy_score(test_language,nb_bigram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Char2Gram Test Prediction Accuracy with Multinomial NB :-',accuracy_score(test_language,nb_char2gram_test_prediction))\n",
    "print('---------------------------------------------------------')\n",
    "print('Char3Gram Test Prediction Accuracy with Multinomial NB :-',accuracy_score(test_language,nb_char3gram_test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(nb_char2gram, open('Char2Gram_nb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(char2gram.get_feature_names(), open('Char2Gram_features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(char2gram, open('vector.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
